{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# David Choi (Choi597)\n",
    "# Prof. Bruno Ribiero\n",
    "# Purdue CS 373 (Data Mining and ML) -> Fake Reviews Categorization/Classification Kaggle Challenge\n",
    "# October 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python Packages for Modeling\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#print(f'pandas version {pd.__version__}')\n",
    "#print(f'Sklearn version {sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.externals import joblib\n",
    "except:\n",
    "    import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handling Data Imports and Parsing\n",
    "test_file = None\n",
    "train_file = None\n",
    "validation_file = None\n",
    "joblib_file = \"LR_model.pkl\"\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "group1 = parser.add_mutually_exclusive_group(required=True)\n",
    "group1.add_argument('-e', '--test', help='Test attributes (to predict)')\n",
    "group1.add_argument('-n', '--train', help='Train data')\n",
    "parser.add_argument('-v', '--validation', help='Validation data')\n",
    "\n",
    "Train = False\n",
    "Test = False\n",
    "Validation = False\n",
    "\n",
    "#Importing Given Python Datasets with Type Initialization\n",
    "file_train = pd.read_csv('reviews_train.csv',quotechar='\"',usecols=[0,1,2,3],dtype={'real review?': int,'category': str, 'rating': int, 'text_': str})\n",
    "file_test = pd.read_csv('reviews_test_attributes.csv',quotechar='\"',usecols=[0,1,2,3,4],dtype={'real review?': int,'category': str, 'rating': int, 'text_': str})\n",
    "file_validation = pd.read_csv('reviews_validation.csv',quotechar='\"',usecols=[0,1,2,3],dtype={'real review?': int,'category': str, 'rating': int, 'text_': str}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37184, 7500)\n"
     ]
    }
   ],
   "source": [
    "#Converting Raw Data into Matrix of TF-IDF (Term Frequency - Inverse Document Frequency) Features\n",
    "vectorizer = TfidfVectorizer(max_features=7500, ngram_range=(1,5))\n",
    "corpora = file_train['text_'].astype(str).values.tolist()\n",
    "vectorizer.fit(corpora)\n",
    "X = vectorizer.transform(corpora)\n",
    "\n",
    "# Dimensions of New Matrix\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 7500)\n"
     ]
    }
   ],
   "source": [
    "#Vectorized TF-IDF on Validation Data\n",
    "corporaVal = file_validation['text_'].astype(str).values.tolist()\n",
    "X1 = vectorizer.transform(corporaVal)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2249, 7500)\n"
     ]
    }
   ],
   "source": [
    "#Vecctorized TF-IDF on Test Data \n",
    "corporaTest = file_test['text_'].astype(str).values.tolist()\n",
    "X2 = vectorizer.transform(corporaTest)\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "#One-Hot Encoding on Category and Rating of Training Data for Features -> Categorical to Numerical\n",
    "fittingC = enc.fit(file_train[['category']])\n",
    "transformingC = fittingC.transform(file_train[['category']]).toarray()\n",
    "fittingR = enc.fit(file_train[['rating']])\n",
    "transformingR = fittingR.transform(file_train[['rating']]).toarray()\n",
    "#Concatenate Vectorized Tf-IDF with Dummies for better Features\n",
    "trainData = np.hstack((X.toarray(), transformingC, transformingR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit-Transform on Training Data\n",
    "fittingCVal = enc.fit(file_validation[['category']])\n",
    "transformingCVal = fittingCVal.transform(file_validation[['category']]).toarray()\n",
    "fittingRVal = enc.fit(file_validation[['rating']])\n",
    "transformingRVal = fittingRVal.transform(file_validation[['rating']]).toarray()\n",
    "valData = np.hstack((X1.toarray(), transformingCVal, transformingRVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittingCTest = enc.fit(file_test[['category']])\n",
    "transformingCTest = fittingCTest.transform(file_test[['category']]).toarray()\n",
    "fittingRTest = enc.fit(file_test[['rating']])\n",
    "transformingRTest = fittingRTest.transform(file_test[['rating']]).toarray()\n",
    "testData = np.hstack((X2.toarray(), transformingCTest, transformingRTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/spack/scholar/fall20/apps/anaconda/2020.11-py38-gcc-4.8.5-djkvkvk/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9842291547950592\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression on Validation Data \n",
    "for C in [10]:\n",
    "    lr = LogisticRegression(penalty=\"l1\", tol=0.001, C=C, fit_intercept=True, solver=\"saga\", intercept_scaling=1, random_state=42)\n",
    "    lr.fit(trainData, file_train['real review?'])\n",
    "    \n",
    "    # Get logistic regression predictions\n",
    "    y_hat = lr.predict_proba(trainData)[:,1]\n",
    "    validationy_hat = lr.predict_proba(valData)[:,1]\n",
    "    accuracy = roc_auc_score(file_validation['real review?'], validationy_hat)\n",
    "    \n",
    "   #accuracy = (yval_pred == file_validation['real review?']).sum() / file_validation['real review?'].size\n",
    "    print(f'Accuracy {accuracy}')\n",
    "    #print(f'Fraction of non-zero model parameters {np.sum(lr.coef_==0)+1}')\n",
    "    \n",
    "    #if accuracy > best_accuracy:\n",
    "        # Save logistic regression model\n",
    "        #joblib.dump(lr, joblib_file)\n",
    "        #best_accuracy = accuracy\n",
    "        \n",
    "    #currlist.append(accuracy)\n",
    "#accList.append(currList)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pitting Model Against Testing Data After Validation Testing\n",
    "y_pred = lr.predict_proba(testData)[:,1]\n",
    "predict = pd.DataFrame(columns = [\"ID\", \"real review?\"])\n",
    "predict[\"ID\"] = range(0, len(file_test))\n",
    "predict[\"real review?\"] = y_pred\n",
    "predict.to_csv(\"predicted_labels.csv\", index = False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Anaconda 2020.11)",
   "language": "python",
   "name": "anaconda-2020.11-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
